##服务器模型##

* C/S模型（Client/Server） C/S模型非常适合资源相对集中的场合，并且它的实现也很简单，但其缺点也很明显：服务器是通信的中心，当访问量过大时，可能所有客户端都将得到很慢的响应。
* P2P模型(Peer to Peer) 点对点，P2P模型使得每台机器在消耗服务器的同时也给别人提供服务，这样资源能够充分，自由地共享。云计算机群可以看做P2P模型的一个典范。但P2P模型的缺点也很明显：当用户之间传输的请求过多时，网络的负载将加重。从编程角度来讲，P2P模型可以看做C/S模型的扩展，每台主机既是客户端，又是服务器。


##模块##

* <font color="red">I/O处理单元</font>是服务器管理客户连接的模块。它通常要完成以下工作：等待并接受新的客户连接，接收客户端数据，将服务器响应数据返回给客户端。但是，数据的收发不一定在I/O处理单元中执行，也可能在逻辑单元中执行，具体在何处执行取决于事件处理模式。（对于一个服务器机群来说，I/O处理单元是一个专门的接入服务器。它实现负载均衡，从所有逻辑服务器中选取负荷最小的一台来为新客户服务。）

* <font color="red">一个逻辑单元</font>通常是一个进程或线程。它分析并处理客户数据，然后将结果传递给I/O处理单元或者直接发送给客户端（具体使用哪种方式取决于事件处理模式）。对服务器机群而言，一个逻辑单元本身就是一台逻辑服务器。服务器通常拥有多个逻辑单元，一实现对多个客户任务的并行处理。

* <font color="red">网络存储单元</font>可以是数据库，缓存和文件，甚至是一台独立的服务器。。

* <font color="red">请求队列</font>是各个单元之间的通信方式的抽象。I/O处理单元接收客户请求时，需要以某种方式通知一个逻辑单元来处理该请求。同样，多个逻辑单元同时访问一个存储单元时，也需要采用某种机制来协调处理竞态条件。请求队列通常被实现为池的一部分。（对服务器机群而言，请求队列是各台服务器之间预先建立的，静态的，永久的TCP连接，这种TCP连接能提高服务器之间交换数据的效率，因为它避免了动态建立TCP连接导致的额外的系统开销。）


I/O模型

* 阻塞I/O  针对阻塞I/O执行的系统调用可能因为无法立即完成而被操作系统挂起，直到等待的时间发生为止。socket的基础API中，可能被阻塞的系统调用包括accept,send,recv和connect。

* 非阻塞I/O  针对非阻塞I/O执行的系统调用则总是立即返回，而不管事件是否已经发生。如果事件没有立即发生，这些系统调用就返回-1，和出错的情况一样，此时我们必须根据errno来区分这两种情况。对accept、send和recv而言，事件未发生时errno通常被设置成EAGAIN（意为“再来一次”）或者EWOULDBLOCK(意为“期望阻塞”)，对connect而言，errno则被设置成EINPROGRESS(意为“在处理中”)。

<font color="red">非阻塞I/O通常要和其他I/O通知机制一起使用。</font>

###I/O通知机制###

* I/O复用  I/O复用是最常使用的I/O通知机制，它指的是，应用程序通过I/O复用函数向内核注册一组事件，内核通过I/O复用函数把其中就绪的事件通知给应用程序。Linux上常用的I/O复用函数是select,poll和epoll_wait。<font color="red">需要指出的是，I/O复用函数本身是阻塞的，它们能提高程序效率的原因在于它们具有同时监听多个I/O事件的能力。</font>

* SIGIO信号  SIGIO信号也可以用来报告I/O事件。

<font color="red">从理论上说，阻塞I/O、I/O复用和信号驱动I/O都是同步I/O模型。因为在这是那种I/O模型中，I/O的读写操作，都是在I/O事件发生之后，由应用程序来完成的。而POSIX规范锁定义的异步I/O模型则不同，对异步I/O而言，用户可以直接对I/O执行读写操作，这些操作告诉内核用户读写缓冲区的位置，以及I/O操作完成之后内核通知应用程序的方式。异步I/O的读写操作总是立即返回，而不论I/O是否是阻塞的，因为真正的读写操作已经由内核接管，也就是说，同步I/O模型要求用户代码自行执行I/O操作（将数据从内核缓冲区读入用户缓冲区，或将数据从用户缓冲区写入内核缓冲区），而异步I/O机制则由内核来执行I/O操作（数据在内核缓冲区和用户缓冲区之间的移动是由内核在"后台"完成的），你可以这样认为，同步I/O向应用程序通知的是I/O就绪事件，而异步I/O向应用程序通知的是I/O完成事件。</font>


###I/O模型对比###

* 阻塞I/O  程序阻塞与读写函数
* I/O复用  程序阻塞于I/O复用系统调用，但可同时监听多个I/O事件，对I/O本身的读写操作是非阻塞的。
* SIGIO信号  信号触发读写就绪事件，用户程序执行读写操作，程序没有阻塞阶段。
* 异步I/O  内核执行读写操作并触发读写完成事件，程序没有阻塞阶段。


###两种高效的事件处理模式###

服务器程序通常需要处理三类事件：I/O事件，信号以及定时事件。

* Reactor   (同步I/O模式通常用于实现Reactor模式)，Reactor是这样一种模式，它要求主线程（I/O处理单元）只负责监听文件描述上是否有事件发生，有的话就立即将该事件通知工作线程（逻辑单元）。除此之外，主线程不做任何其他性质的工作。读写数据，接受新的连接，以及处理客户请求均在工作线程中完成。

* Proactor  (异步I/O模式通常用于实现Proactor模式)


使用同步I/O模型（以epoll_wait为例）实现Reactor模式的工作流程是：

* 主线程往epoll内核事件表中注册socket上的读就绪事件。
* 主线程调用epoll_wait等待socket上有数据可读。
* 当socket上有数据可读时，epoll_wait通知主线程，主线程将socket可读事件放入请求队列。
* 睡眠在请求队列上的某个工作线程被唤醒，它从socket读取数据，并处理客户请求然后往epoll内核事件表中注册该socket上的写就绪事件。
* 主线程调用epoll_wait等待socket可写。
* 当socket可写时，epoll_wait通知主线程，主线程将socket可写事件放入请求队列。
* 睡眠在请求队列上的某个工作线程被唤醒，它往socket上写入服务器处理客户请求的结果。

工作线程从请求队列中取出事件后，想根据事件的类型来决定如何处理它，对于可读事件，执行读数据和处理请求的操作：对于可写事件，执行写数据的操作。因此Reactor模式没有必要区分所谓的“读工作线程”和“写工作线程” 。

Proactor模式将所有I/O操作都交给主线程和内核来处理，工作线程仅仅负责业务逻辑。

使用异步I/O模型（以aio_read和aio_write为例）实现的Proactor模式的工作流程是：

* 主线程调用aio_read函数向内核注册socket上的读完成事件，并告诉内核用户读缓冲区的位置，以及读操作完成时如何通知应用程序。

* 主线程继续处理其他逻辑。

* 当socket上的数据被读入用户缓冲区后，内核将向应用程序发送一个信号，以通知应用程序数据已经可用。

* 应用程序预先定义好的信号处理函数选择一个工作线程来处理客户请求，工作线程处理完客户请求后，调用aio_write函数向内核注册socket上的写完成事件，并告诉内核用户写缓冲区的位置，以及写操作完成时如何通知应用程序。

* 主线程继续处理其他逻辑。

* 当用户缓冲区的数据被写入socket之后，内核将向应用程序发送一个信号，以通知应用程序数据已经发送完毕。

* 应用程序预先定义好的信号处理函数选择一个工作线程来做善后处理。比如决定是否关闭socket。


###常见的各种池###

* 内存池  内存池通常用于socket的接收缓存和发送缓存

* 进程池和线程池都是并发编程常用的"伎俩",当我们需要一个工作进程或工作线程来处理新到来的客户请求时，我们可以直接从进程池或线程池中取得一个执行实体，而无需动态地调用fork或pthread_create等函数来穿件进程和线程。

* 连接池通常用于服务器或服务器机群的内部永久连接。

<font color="red">为每个客户端连接都创建一个工作线程的服务器模型是不可取的。</font>


###提高服务器性能的建议###

* I/O复用
* 有限状态机
* 善用各种池
* 尽量避免数据复制
* 减少上下文切换（不宜使用过多进程或线程）和锁的使用（尽量不用，或者减少锁的粒度，使用读写锁）


##I/O复用##

<font color="red">I/O复用（select,poll,epoll）虽然能同时监听多个文件描述符，但它本身是阻塞的。并且当多个文件描述符同时就绪时，如果不采取措施，程序就只能按顺序一次处理其中的每一个文件描述符，这使得服务器程序看起来像是串行工作的。如果要实现并发，只能使用多线程或多进程等编程手段。</font>

select的使用例子:

```
int fd1, fd2;         /* 在定义两个描述符*/
fd1 = socket(...);    /* 创建socket连接*/
fd2 = open(“/dev/tyS0”,O_RDWR); /* 打开一个串口*/
FD_ZERO(&rfds);       /* 用select函数之前先把集合清零 */
FD_SET(fd1, &rfds);   /* 分别把2个描述符加入读监视集合里去 */
FD_SET(fd2, &rfds);
int maxfd = 0;
maxfd = (fd1>fd2)?(fd1+1):(fd2+1);           /* 注意是最大值还要加1 */
ret = select(maxfd, &rfds, NULL, NULL, &tv); /*然后调用select函数*/
switch（ret）
{
case -1：perror("select");/* 这说明select函数出错 */
case 0：printf("超时/n"); /* 说明在设定的时间内，socket的状态没有发生变化 */
default：
if（FD_ISSET(fd1, &rfds)） 处理函数1（）；/*socket有数据来*/
if（FD_ISSET(fd2, &rfds)） 处理函数2（）；/*ttyS0有数据来*/
}

```

fd_set结构体仅包含一个整型数组该数组的每个元素的每一位（bit）标记一个文件描述符。fd_set能容纳的文件描述符数量由FD_SETSIZE指定，这就限制了select能同时处理的文件描述符的总量。
由于位操作过于繁琐，我们应该使用下面的一系列宏来访问fd_set结构体中的位：

```
FD_ZERO(fd_set *fdset);/*清除fdset的所有位*/
FD_SET(int fd,fd_set *fdset);/*设置fdset的位fd*/
FD_CLR(int fd,fd_set *fdset);/*清除fdset的位fd*/
int FD_ISSET(int fd,fd_set *fdset);/*测试fdset的位fd是否被设置*/

```

###epoll###

```
  #include <sys/epoll.h>
  
  int epoll_create(int size);
  
  int epoll_ctl(int epfd,int op,int fd,struct epoll_event *event);
  
  int epoll_wait(int epfd,struct epoll_event* events,int maxevents,int timeout);
  
```


<font color="red">首先,epoll使用一组函数来完成任务，而不是单个函数。其次，epoll把用户关心的文件描述符上的事件放在内核里的一个事件表中，从而无须像select和poll那样每次调用都要重复传入文件描述符或事件集。但epoll需要一个额外的文件描述符，来唯一标识内核中的这个事件表。这个文件描述符使用epoll_create来创建。返回int epfd</font>

操作epoll的内核事件表:

```
int epoll_ctl(int epfd,int op,int fd,strct epoll_event *event);

```

fd参数是要操作的文件描述符，op参数则指定操作类型，可选值为:

* EPOLL _ CTL _ ADD   往事件表中注册fd上的事件
* EPOLL _ CTL _ MOD   修改fd上的注册事件
* EPOLL_ CTL _ DEL   删除fd上的注册事件

event参数指定事件


<font color="red">epoll对文件描述符的操作有两种模式：LT(Level Trigger,电平出发)模式和ET(Edge Trigger,边缘出发)模式。LT是默认的工作模式。</font>

<font color="red">对于采用LT工作模式的文件描述符，当epoll_wait检测到其上有事件发生并将此事件通知应用程序后，应用程序可以不立即处理该事件。这样，当应用程序下一次调用epoll_wait时，epoll_wait还会再次向应用程序通告此事件，直到该事件被处理。而对于采用ET工作模式的文件描述符，当epoll_wait检测到其上有事件发生并将此事件通知应用程序后，应用程序必须立即处理该事件，因为后续的epoll_wait调用将不再向应用程序通知这一事件。ET模式为高效工作模式。特别要注意：每个使用ET模式的文件描述符都应该是非阻塞的。</font>


###EPOLLONESHOT事件###

<font color="red">对于注册了EPOLLONESHOT事件的文件描述符，操作系统最多出发其上注册的一个可读，可写或者异常事件，且只触发一次，除非我们使用epoll_ctl函数重置该文件描述符上注册的EPOLLONESHOT事件。这样，当一个线程在处理某个socket时，其他线程是不可能又机会操作该socket的。但反过来思考，注册了EPOLLONESHOT事件的socket一旦被某个线程处理完毕，该线程就应该立即重置这个socket上的EPOLLONESHOT事件，以确保这个socket下一次可读时，其EPOLLIN事件能被触发，进而让其他工作线程有机会继续处理这个socket.</font>







































