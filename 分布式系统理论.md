```
WARO(Write All Read one)是一种简单的副本控制协议，当Client请求向某副本写数据时(更新数据)，只有当所有的副本都更新成功之后，这次写操作才算成功，否则视为失败。
```

```
从这里可以看出两点：①写操作很脆弱，因为只要有一个副本更新失败，此次写操作就视为失败了。②读操作很简单，因为，所有的副本更新成功，才视为更新成功，从而保证所有的副本一致。这样，只需要读任何一个副本上的数据即可。假设有N个副本，N-1个都宕机了，剩下的那个副本仍能提供读服务；但是只要有一个副本宕机了，写服务就不会成功。
```

```
 WARO牺牲了更新服务的可用性，最大程度地增强了读服务的可用性。
```

```
Quorum就是更新服务和读服务之间进行一个折衷。
Quorum机制是“抽屉原理”的一个应用。定义如下：假设有N个副本，更新操作wi 在W个副本中更新成功之后，才认为此次更新操作wi 成功。称成功提交的更新操作对应的数据为：“成功提交的数据”。对于读操作而言，至少需要读R个副本才能读到此次更新的数据。其中，W+R>N ，即W和R有重叠。一般，W+R=N+1
```

```
假设系统中有5个副本，W=3，R=3。初始时数据为(V1，V1，V1，V1，V1）--成功提交的版本号为1

当某次更新操作在3个副本上成功后，就认为此次更新操作成功。数据变成：(V2，V2，V2，V1，V1）--成功提交后，版本号变成2

因此，最多只需要读3个副本，一定能够读到V2(此次更新成功的数据)。而在后台，可对剩余的V1 同步到V2，而不需要让Client知道。
```

```
分布式系统中常见的三种一致性模型

①强一致性：当更新操作在某个副本上执行成功后，之后所有的读操作都要能够获得最新的数据。对于单副本而言，读写操作都是在同一数据上执行，很容易保证一致性；而对于多副本数据，则需要使用分布式协议如2PC协议。

②弱一致性：当更新某数据时，用户读到最新的数据需要一段时间。

③最终一致性：它是一种特殊形式的弱一致性。它不能保证当某个数据Ｘ更新后，在所有后续对Ｘ的操作能够看到新数据，而是需要一个时间片段，在经过该时间片段之后，则能保证。在这个时间片段内，数据可能是不一致的，该片段称“不一致窗口“。
```

```
Raft首先会选举出一个唯一的Leader， Leader负责管理日志，所用对日志的添加和状态变化操作都通过Leader完成。Leader接受用户的日志请求并将日志分发给系统中其他节点。并告知其他节点何时可以安全地将日志应用到状态机上。这种方式简化了多副本日志的管理，日志的数据流向是从Leader到其他节点，而其他节点不会发送日志给Leader,非常简单。Leader当掉时，Raft会选出一个新的Leader。

通过采用Leader, Raft把一致性问题分解为相互独立的3个子问题：

Leader选举：系统初始化时需要选出Leader, Leader当掉时选出新的Leader。

日志分发：Leader接受日志请求并分发给其他节点。

确保正确性：确保所有日志副本是一致的。
```

```
在Raft集群中，任何一个节点在任何一个时刻都处于三种状态之一：Leader, Follower, Candidate。在系统正常运行的绝大部分时间，系统中会有一个Leader, 其他都是Follower。由Leader接受用户的请求，每个用户请求视为一项日志。在Raft语境下，我们所说的日志并不是系统运行的错误，警告，Debug信息等，而是指操作记录。例如提供键值对存储的Raft集群，日志是指 "设置A=100",  "设置B=3", "删除C"这种引起状态变化的键值对的操作， 而查看A的值并不是日志，因为它不会引起系统状态的变化。Follower是被动的，只接受来自Leader和Candidate的请求，自己不会发出任何请求。Candidate状态是在Leader选举过程中的临时状态。
```

```
任期
Raft把时间分为连续的任期(term)，每个任期可以是任意时长，任期用连续的整数进行标号。每个任期首先进行Leader选举，选举时，多个Candidate竞争成为Leader,一旦某个节点成为Leader,其他节点则变回Follower，成为Leader的节点将在该任期内一致担任Leader，如果该Leader节点发生故障，其他节点会在新的任期内进行选举。任何一个任期内都不会有多个Leader。Raft系统中，任期是一个及其重要的概念，每个节点都维护着当前任期的值，每次节点间的通信都包含任期信息，每个节点在检测到自己的任期值低于其他节点，都会更新自己的任期值，设置为检测到的较高的值。当Leader和Candidate发现自己的任期低于别的节点，则立即把自己转换为Follower。
```

```
RPC
Raft节点间的通信采用RPC调用， Raft算法核心部分只需要用到两个RPC: RequestVote和AppendEntries。RequestVote RPC由Candidate发起用于Leader选举。AppendEntries RPC由Leader发起，用于心跳和日志复制。而Follower不会发起任何RPC。
```

```
Leader选举过程
Raft采用心跳机制触发Leader选举。当系统启动时，所有节点初始化为Follower状态，设置任期为0，并启动计时器，计时器超时后，Follower节点转化为Candidate节点，一旦转化为Candidate节点，立即开始一下几件事情：

1.增加自己的任期数

2. 启动一个新的计时器

3. 给自己投一票

4.向所有其他节点发送RequestVote RPC请求，并等待其他节点回复。

如果在计时器超时前接收到多数节点的同意投票，则转换为Leader。如果接受到其他节点的AppendEntries心跳RPC，说明其他节点已经被选为Leader, 则转换为Follower。如果计时器超时时还没有接受到以上两种信息中的任何一种，则重复步骤1-4，进行新的选举。

节点在接受到多数节点的投票成为Leader后，会立即向所有节点发送AppendEntries 心跳RPC。所有Candidate收到心跳RPC后，转换为Follower，选举结束。

每个Follower在一个任期内只能投一票，采取先到先得的策略。每个Follower有一个计时器，在计时器超时时仍然没有接受到来自Leader的心跳RPC, 则转换为Candidate, 开始请求投票。也就是在当期Leader当掉后，就会有Follower开始转换为Candidate开始投票。

如果多个节点同时发起投票，每个节点都没有拿到多数票（这种情况成为Split Vote），则增加任期数，在新的任期内重新进行投票。有没有可能Split Vote反复发生，永远都选不出Leader呢？

Raft采取随机超时时间，Raft系统有一个选举超时配置项，Follower和Candidate的计时器超时时间每次重新计算，随机选取配置时间的1倍到2倍之间。即使所有节点同时启动，由于随机超时时间的设置，各个节点一般不会同时转为Candidate，先转为Candidate的节点会先发起投票，从而获得多数票。因而在每个任期内，多个节点同时请求投票并且都只获得少数票的几率很小，连续多次发生这种情况几率更小，理论上几率很小，实际上可以认为完全不可能发生。在我的Raft实现中，基本上都在1-2个任期内选出Leader。
```

